{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import torch\n",
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "#page_py = wiki_wiki.page('Python_(programming_language)')\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "'''科学上网代理设置'''\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \n",
    "\n",
    "        \n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        \n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                s = self.model(**chunk)\n",
    "                answer_start_scores = s.start_logits\n",
    "                answer_end_scores = s.end_logits\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  \n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  \n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Barack Obama born?\n",
      "Barack Obama Sr.\n",
      "Top wiki result: Barack Obama Sr. (id: ??, ns: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2997 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 18 June 1934 / August 1961 / 4 August 1961 / \n",
      "\n",
      "Question: Why is the sky blue?\n",
      "Diffuse sky radiation\n",
      "Top wiki result: Diffuse sky radiation (id: ??, ns: 0)\n",
      "Answer: Rayleigh scattering / its intrinsic nature, can illuminate under - canopy leaves permitting more efficient total whole - plant photosynthesis than would otherwise be the case, and also increasing evaporative cooling from vegetated surfaces / \n",
      "\n",
      "Question: What is the python\n",
      "Python (programming language)\n",
      "Top wiki result: Python (programming language) (id: ??, ns: 0)\n",
      "Answer: pythonic / Python 3 variants / IronPython allows running Python 2. 7 programs / [CLS] What is the python [SEP] \" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python 3. 9 / Pygame / Pypi. python. org / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'When was Barack Obama born?',\n",
    "    'Why is the sky blue?',\n",
    "    'What is the python'\n",
    "]\n",
    "\n",
    "reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "\n",
    "# if you trained your own model using the training cell earlier, you can access it with this:\n",
    "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    results = wiki.search(question)\n",
    "    print(results[0])\n",
    "    str = results[0]\n",
    "    page = wiki_wiki.page(str)\n",
    "    print(f\"Top wiki result: {page}\")\n",
    "\n",
    "    text = page.text\n",
    "\n",
    "    reader.tokenize(question, text)\n",
    "    print(f\"Answer: {reader.get_answer()}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
